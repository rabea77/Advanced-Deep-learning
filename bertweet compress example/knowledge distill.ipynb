{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12788188,"sourceType":"datasetVersion","datasetId":8085117},{"sourceId":530303,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":414844,"modelId":432604},{"sourceId":529722,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":414380,"modelId":432130}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom torch.nn import functional as F\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:43:44.862131Z","iopub.execute_input":"2025-08-21T17:43:44.862754Z","iopub.status.idle":"2025-08-21T17:44:11.159101Z","shell.execute_reply.started":"2025-08-21T17:43:44.862731Z","shell.execute_reply":"2025-08-21T17:44:11.158546Z"}},"outputs":[{"name":"stderr","text":"2025-08-21 17:43:58.002279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755798238.183012      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755798238.242017      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# Cell 1 â€” Data + labels (mirror teacher encoding)\n# =========================\nimport pandas as pd\nimport torch, random, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import set_seed, AutoConfig\n\n# --- paths & columns ---\nDATA_PATH   = r\"/kaggle/input/edaaaaa-tergol5/my_eda.csv\"\nTEXT_COL    = \"Tweet\"\nTARGET_COL  = \"Sentiment\"\nTEACHER_DIR = \"/kaggle/input/covidbertttttttttt/pytorch/default/1\"  # <-- use this\n\n# --- seeds ---\nSEED = 42\nset_seed(SEED)\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\nMAX_LENGTH = 128\n\n# --------- Load CSV ---------\ndf = pd.read_csv(DATA_PATH, encoding=\"latin1\")\n\n# Keep original text if present; use normalized for training\nif \"Tweet\" in df.columns:\n    df = df.rename(columns={\"Tweet\": \"Original\"})\nif \"normalized_tweet\" in df.columns:\n    df = df.rename(columns={\"normalized_tweet\": \"Tweet\"})\n\nassert TEXT_COL in df.columns and TARGET_COL in df.columns, f\"Missing {TEXT_COL}/{TARGET_COL} in CSV.\"\n\n# --------- Load teacher label maps from the valid folder ---------\ncfg = AutoConfig.from_pretrained(TEACHER_DIR)\nlabel2id = dict(cfg.label2id or {})\nid2label = dict(cfg.id2label or {})\n\n# If the teacher config doesn't define them, derive from data (stable order)\nif not label2id or not id2label:\n    uniq = sorted(df[TARGET_COL].astype(str).unique())\n    label2id = {lbl: i for i, lbl in enumerate(uniq)}\n    id2label = {i: lbl for lbl, i in label2id.items()}\n\ndef to_label_id(y):\n    # pass-through if already an int id\n    try:\n        yi = int(y)\n        if yi in id2label:\n            return yi\n    except Exception:\n        pass\n    ys = str(y)\n    if ys not in label2id:\n        raise ValueError(f\"Label '{y}' not in mapping: {list(label2id.keys())}\")\n    return label2id[ys]\n\ndf[\"label\"] = df[TARGET_COL].apply(to_label_id)\nnum_labels = len(id2label)\n\nprint(\"Teacher label2id:\", label2id)\nprint(\"id2label:\", id2label)\nprint(\"num_labels =\", num_labels)\nprint(\"Label dist:\\n\", df[\"label\"].value_counts().sort_index())\n\n# --------- Stratified split ---------\ntrain_df, eval_df = train_test_split(\n    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n)\nprint(\"Train/Eval sizes:\", len(train_df), len(eval_df))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:44:57.355819Z","iopub.execute_input":"2025-08-21T17:44:57.356377Z","iopub.status.idle":"2025-08-21T17:44:57.968215Z","shell.execute_reply.started":"2025-08-21T17:44:57.356355Z","shell.execute_reply":"2025-08-21T17:44:57.967433Z"}},"outputs":[{"name":"stdout","text":"Teacher label2id: {'Extremely Negative': 0, 'Extremely Positive': 1, 'Negative': 2, 'Neutral': 3, 'Positive': 4}\nid2label: {0: 'Extremely Negative', 1: 'Extremely Positive', 2: 'Negative', 3: 'Neutral', 4: 'Positive'}\nnum_labels = 5\nLabel dist:\n label\n0     5480\n1     6619\n2     9907\n3     7673\n4    11413\nName: count, dtype: int64\nTrain/Eval sizes: 32873 8219\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# =========================\n# 2) Build HF DatasetDict\n# =========================\ncols = [TEXT_COL, \"label\"]\ntrain_ds = Dataset.from_pandas(train_df[cols].rename(columns={\"label\": \"labels\"}), preserve_index=False)\neval_ds  = Dataset.from_pandas(eval_df[cols].rename(columns={\"label\": \"labels\"}),  preserve_index=False)\nraw_datasets = DatasetDict({\"train\": train_ds, \"test\": eval_ds})\n\n# =========================\n# 3) Tokenizer (teacher) & tokenization\n# =========================\nteacher_NAME = \"/kaggle/input/covidbertttttttttt/pytorch/default/1\"  # <-- set your student here\ntokenizer = AutoTokenizer.from_pretrained(teacher_NAME, use_fast=True)\n\nMAX_LENGTH = 96  # or whatever you used in Cell 1\n\ndef tokenize_function(batch):\n    return tokenizer(\n        batch[TEXT_COL],   # uses TEXT_COL from global\n        truncation=True,\n        max_length=MAX_LENGTH,\n    )\n\ntokenized_datasets = raw_datasets.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[TEXT_COL],\n)\n\n# Ensure expected columns\nassert \"labels\" in tokenized_datasets[\"train\"].column_names\n\n# Dynamic padding collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Torch format\ntokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# =========================\n# Metrics (5-class safe)\n# =========================\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):  # some trainers return (logits, ...)\n        preds = preds[0]\n    preds = preds.argmax(-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n    }\n\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:02.514266Z","iopub.execute_input":"2025-08-21T17:45:02.514578Z","iopub.status.idle":"2025-08-21T17:45:06.610306Z","shell.execute_reply.started":"2025-08-21T17:45:02.514544Z","shell.execute_reply":"2025-08-21T17:45:06.609758Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e9bf8283d34399b834f93cb51de6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8219 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f16edb134aa844c489b10c3e225e7cbe"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 32873\n    })\n    test: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 8219\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/input/finetuned-bertweet/pytorch/default/1/finetuned_from_old\").to(device)\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:08.950457Z","iopub.execute_input":"2025-08-21T17:45:08.950967Z","iopub.status.idle":"2025-08-21T17:45:20.855774Z","shell.execute_reply.started":"2025-08-21T17:45:08.950941Z","shell.execute_reply":"2025-08-21T17:45:20.855006Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# define compute_metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    accuracy = (preds == labels).astype(float).mean().item()\n    return {\"accuracy\": accuracy}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:23.182080Z","iopub.execute_input":"2025-08-21T17:45:23.182613Z","iopub.status.idle":"2025-08-21T17:45:23.187019Z","shell.execute_reply.started":"2025-08-21T17:45:23.182586Z","shell.execute_reply":"2025-08-21T17:45:23.186193Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    pad_to_multiple_of=8 if torch.cuda.is_available() else None,  # nice for fp16\n)\n\nargs = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    num_train_epochs=1,\n    save_strategy=\"no\",\n    fp16=torch.cuda.is_available(),\n    dataloader_pin_memory=True,\n    dataloader_num_workers=2,\n    report_to=[],\n    logging_strategy=\"no\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"].select(range(100)),\n    eval_dataset=tokenized_datasets[\"test\"].select(range(100)),\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,   # <<<<<< IMPORTANT\n)\n\ntrainer.train()\nmetrics = trainer.evaluate()             # <- show eval after training\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:25.656677Z","iopub.execute_input":"2025-08-21T17:45:25.657363Z","iopub.status.idle":"2025-08-21T17:45:31.720203Z","shell.execute_reply.started":"2025-08-21T17:45:25.657337Z","shell.execute_reply":"2025-08-21T17:45:31.719445Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.569053</td>\n      <td>0.230000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.5690526962280273, 'eval_accuracy': 0.23, 'eval_runtime': 0.9739, 'eval_samples_per_second': 102.675, 'eval_steps_per_second': 2.054, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Train a small student model using a larger teacher\nteacher = model\nstudent = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=5) # Smaller model of my choice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:42.202228Z","iopub.execute_input":"2025-08-21T17:45:42.202500Z","iopub.status.idle":"2025-08-21T17:45:44.332757Z","shell.execute_reply.started":"2025-08-21T17:45:42.202478Z","shell.execute_reply":"2025-08-21T17:45:44.332197Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98adff58e5f64fa48afe4fb03d44b03e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1446ef1a1c44b1a89db1e99b0cd6206"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        outputs_student = model(**inputs)\n        with torch.no_grad():\n            outputs_teacher = self.teacher(**inputs)\n\n        loss_ce = F.cross_entropy(outputs_student.logits, inputs[\"labels\"])\n        loss_kl = F.kl_div(\n            F.log_softmax(outputs_student.logits / self.temperature, dim=-1),\n            F.softmax(outputs_teacher.logits / self.temperature, dim=-1),\n            reduction=\"batchmean\") * (self.temperature ** 2)\n        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n        \n        return (loss, outputs_student) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:46.659230Z","iopub.execute_input":"2025-08-21T17:45:46.660014Z","iopub.status.idle":"2025-08-21T17:45:46.666370Z","shell.execute_reply.started":"2025-08-21T17:45:46.659988Z","shell.execute_reply":"2025-08-21T17:45:46.665548Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# revert to Python lists (no framework formatting)\ntokenized_datasets.reset_format()          # <- or: tokenized_datasets.set_format(type=None)\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n)\n\ntrainer_distill = DistillationTrainer(\n    model=student,\n    teacher_model=teacher,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator\n)\n\ntrainer_distill.train()\nprint(\"\\nDistillation complete. Student model trained.\")\nprint(\"Student model size:\", sum(p.numel() for p in student.parameters()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:45:49.844387Z","iopub.execute_input":"2025-08-21T17:45:49.844680Z","iopub.status.idle":"2025-08-21T17:53:42.684726Z","shell.execute_reply.started":"2025-08-21T17:45:49.844656Z","shell.execute_reply":"2025-08-21T17:53:42.683843Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2055' max='2055' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2055/2055 07:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.513866</td>\n      <td>0.834043</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\nDistillation complete. Student model trained.\nStudent model size: 66957317\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"OUTPUT_DIR = \"./student_model\"\n\nstudent.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Saved student model + tokenizer to\", OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:55:43.981464Z","iopub.execute_input":"2025-08-21T17:55:43.981826Z","iopub.status.idle":"2025-08-21T17:55:44.591581Z","shell.execute_reply.started":"2025-08-21T17:55:43.981798Z","shell.execute_reply":"2025-08-21T17:55:44.590717Z"}},"outputs":[{"name":"stdout","text":"Saved student model + tokenizer to ./student_model\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os, shutil\n\n# 1. Pick an output folder\nOUTPUT_DIR = \"/kaggle/working/student_model\"\n\n# 2. Save model + tokenizer\nstudent.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# 3. Zip it up\nshutil.make_archive(OUTPUT_DIR, \"zip\", OUTPUT_DIR)\n\nprint(\"Saved + zipped to:\", OUTPUT_DIR + \".zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T17:55:47.488528Z","iopub.execute_input":"2025-08-21T17:55:47.489343Z","iopub.status.idle":"2025-08-21T17:56:01.603005Z","shell.execute_reply.started":"2025-08-21T17:55:47.489311Z","shell.execute_reply":"2025-08-21T17:56:01.602222Z"}},"outputs":[{"name":"stdout","text":"Saved + zipped to: /kaggle/working/student_model.zip\n","output_type":"stream"}],"execution_count":11}]}