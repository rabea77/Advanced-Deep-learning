{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-11T15:33:41.686329Z",
     "iopub.status.busy": "2025-08-11T15:33:41.685684Z",
     "iopub.status.idle": "2025-08-11T15:33:41.834659Z",
     "shell.execute_reply": "2025-08-11T15:33:41.834074Z",
     "shell.execute_reply.started": "2025-08-11T15:33:41.686292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "wandb.login(key=\"d12da696b882ebdf6b786d182d46febc1a77dcdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:33:41.836164Z",
     "iopub.status.busy": "2025-08-11T15:33:41.835931Z",
     "iopub.status.idle": "2025-08-11T15:33:42.279702Z",
     "shell.execute_reply": "2025-08-11T15:33:42.278921Z",
     "shell.execute_reply.started": "2025-08-11T15:33:41.836147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → Extremely Negative\n",
      "1 → Extremely Positive\n",
      "2 → Negative\n",
      "3 → Neutral\n",
      "4 → Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib  # לשמירת ה-LabelEncoder (אופציונלי)\n",
    "\n",
    "# --- טעינת הדאטה ---\n",
    "df = pd.read_csv(\"/kaggle/input/traindataset/processed_train.csv\")\n",
    "\n",
    "# שינוי שם עמודת הטקסט\n",
    "df = df.rename(columns={'fully_clean_text': 'Tweet'})\n",
    "\n",
    "# קידוד התוויות ממחרוזות למספרים\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Sentiment'])\n",
    "\n",
    "# שמירת המיפוי לשימוש עתידי (אופציונלי)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# הצגת המיפוי\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} → {label}\")\n",
    "\n",
    "# --- חלוקה ל-Train / Eval \n",
    "train_df, eval_df = train_test_split(\n",
    "    df[['Tweet', 'label']],          # keep only what the model needs\n",
    "    test_size=0.2,                   # 80/20 split; change if you like\n",
    "    random_state=42,\n",
    "    stratify=df['label']             # keep class balance\n",
    ")\n",
    "\n",
    "# שמירה של רק העמודות הדרושות למודל\n",
    "train_df = train_df[['Tweet', 'label']]\n",
    "eval_df = eval_df[['Tweet', 'label']]\n",
    "\n",
    "\n",
    "# שמירת קבצים\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "eval_df.to_csv(\"eval_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:33:42.280675Z",
     "iopub.status.busy": "2025-08-11T15:33:42.280412Z",
     "iopub.status.idle": "2025-08-11T15:34:10.833490Z",
     "shell.execute_reply": "2025-08-11T15:34:10.832462Z",
     "shell.execute_reply.started": "2025-08-11T15:33:42.280658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f1cf20707e4fb284ca121ca3f3917b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6b57f7b39e45d28b545f29178b82d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3d54cf7cb9425195ff4cab6b9177e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb34284e5ef84168bc33f4f99ad979be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:33:53.126895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754926433.318829      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754926433.380453      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e95da5ce6c1428493cfa149db5605d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1167318af6b4b7280f2cb82547fb8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#לשקול להחזיר את אמוג'ים, hashtags, mentions############\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# נשתמש במודל BERTweet-Large של VinAI\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "\n",
    "# חשוב! BERTweet לא תומך בטוקניזר המהיר (Fast tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# טען את המודל למטרת Classification עם 5 תוויות\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "\n",
    "model  # הצגת מבנה המודל\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:10.836636Z",
     "iopub.status.busy": "2025-08-11T15:34:10.836065Z",
     "iopub.status.idle": "2025-08-11T15:34:10.843447Z",
     "shell.execute_reply": "2025-08-11T15:34:10.842393Z",
     "shell.execute_reply.started": "2025-08-11T15:34:10.836614Z"
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.texts = dataframe['Tweet'].tolist()\n",
    "        self.labels = dataframe['label'].tolist() \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=110,  \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:10.845438Z",
     "iopub.status.busy": "2025-08-11T15:34:10.844782Z",
     "iopub.status.idle": "2025-08-11T15:34:10.999891Z",
     "shell.execute_reply": "2025-08-11T15:34:10.998817Z",
     "shell.execute_reply.started": "2025-08-11T15:34:10.845375Z"
    }
   },
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_acc, best_acc_epoch, current_acc, current_epoch):\n",
    "    \"\"\"\n",
    "    עצירה מוקדמת לפי Val Accuracy בלבד.\n",
    "    מחזיר: best_acc, best_acc_epoch, early_stop_flag\n",
    "    \"\"\"\n",
    "    early_stop_flag = False\n",
    "    if current_acc > best_acc:\n",
    "        best_acc = current_acc\n",
    "        best_acc_epoch = current_epoch\n",
    "    elif current_epoch - best_acc_epoch > patience:\n",
    "        early_stop_flag = True\n",
    "    return best_acc, best_acc_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:11.000998Z",
     "iopub.status.busy": "2025-08-11T15:34:11.000798Z",
     "iopub.status.idle": "2025-08-11T15:34:11.017721Z",
     "shell.execute_reply": "2025-08-11T15:34:11.016877Z",
     "shell.execute_reply.started": "2025-08-11T15:34:11.000981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2 3 4]\n",
      "Class weights: [1.49848801 1.2401726  0.82850966 1.07523382 0.71978462]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# חישוב משקלי תוויות לפי הופעה בפועל — על ה-TRAIN בלבד\n",
    "train_labels = train_df['label'].values\n",
    "classes = np.unique(train_labels)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# הפיכה לטנסור לשימוש בתוך CrossEntropyLoss\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "# פונקציית הפסד עם משקלים\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# אופציונלי: להדפיס כדי לדעת מה קיבלת\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:11.019211Z",
     "iopub.status.busy": "2025-08-11T15:34:11.018900Z",
     "iopub.status.idle": "2025-08-11T15:34:11.035674Z",
     "shell.execute_reply": "2025-08-11T15:34:11.034719Z",
     "shell.execute_reply.started": "2025-08-11T15:34:11.019184Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_accuracy_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            total_train_samples += input_ids.size(0)\n",
    "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * input_ids.size(0)\n",
    "                total_val_samples += input_ids.size(0)\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "            patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch)\n",
    "\n",
    "        if val_accuracy == best_val_accuracy:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Validation F1\": val_f1\n",
    "        })\n",
    "\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
    "\n",
    "    return best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:11.037891Z",
     "iopub.status.busy": "2025-08-11T15:34:11.037146Z",
     "iopub.status.idle": "2025-08-11T15:34:11.060376Z",
     "shell.execute_reply": "2025-08-11T15:34:11.059148Z",
     "shell.execute_reply.started": "2025-08-11T15:34:11.037864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    # === Hyperparameter suggestions ===\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 2, 5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "\n",
    "    # === Tokenizer and Dataset ===\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", use_fast=False)\n",
    "\n",
    "    train_dataset = TweetDataset(train_df, tokenizer)\n",
    "    val_dataset = TweetDataset(eval_df, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # === Load BERTweet Model ===\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"vinai/bertweet-large\", num_labels=5\n",
    "    ).to(device)\n",
    "\n",
    "    # === Freeze all layers first ===\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # === Unfreeze the last `num_layers` of encoder ===\n",
    "    for param in model.roberta.encoder.layer[-num_layers:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # === Unfreeze the classification head ===\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # === Define loss with class weights ===\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # === Optimizer ===\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # === Initialize W&B for tracking ===\n",
    "    wandb.init(\n",
    "        project=\"bertweet-sentiment\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"patience\": patience,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"architecture\": \"BERTweet-Large\",\n",
    "            \"dataset\": \"covid-tweets\"\n",
    "        },\n",
    "        name=f\"trial_{trial.number}\"\n",
    "    )\n",
    "\n",
    "    # === Train and evaluate ===\n",
    "    best_val_accuracy = train_model_with_hyperparams(\n",
    "        model, train_loader, val_loader, optimizer, criterion, epochs=10,\n",
    "        patience=patience, trial=trial\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T15:34:11.062122Z",
     "iopub.status.busy": "2025-08-11T15:34:11.061708Z",
     "iopub.status.idle": "2025-08-11T20:42:52.808335Z",
     "shell.execute_reply": "2025-08-11T20:42:52.807635Z",
     "shell.execute_reply.started": "2025-08-11T15:34:11.062093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:34:11,078] A new study created in memory with name: BERT_Accuracy_Study\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_153413-dc6ykvp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dc6ykvp1' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dc6ykvp1' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dc6ykvp1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▅▄█▇██▇</td></tr><tr><td>Validation F1</td><td>▁▅▅▅▄█▇██▇</td></tr><tr><td>Validation Loss</td><td>▆▃▂▂▅▁▄▄▆█</td></tr><tr><td>Validation Precision</td><td>▁▄▆▅▅█▇███</td></tr><tr><td>Validation Recall</td><td>▁▄▅▅▄█▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.87829</td></tr><tr><td>Train Loss</td><td>0.28987</td></tr><tr><td>Validation Accuracy</td><td>0.67949</td></tr><tr><td>Validation F1</td><td>0.67449</td></tr><tr><td>Validation Loss</td><td>1.01193</td></tr><tr><td>Validation Precision</td><td>0.68735</td></tr><tr><td>Validation Recall</td><td>0.67949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dc6ykvp1' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dc6ykvp1</a><br> View project at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250811_153413-dc6ykvp1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:45:03,664] Trial 0 finished with value: 0.6950803701899659 and parameters: {'learning_rate': 4.468780967980255e-05, 'weight_decay': 2.7116136370918185e-05, 'patience': 2, 'batch_size': 128, 'num_layers': 3}. Best is trial 0 with value: 0.6950803701899659.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_164506-nmj4d2ci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/nmj4d2ci' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/nmj4d2ci' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/nmj4d2ci</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▄▅▆▆██▇▇</td></tr><tr><td>Validation F1</td><td>▁▂▄▅▆▅██▇▇</td></tr><tr><td>Validation Loss</td><td>█▆▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▃▄▅▆▆█▇█▇</td></tr><tr><td>Validation Recall</td><td>▁▁▄▅▆▆██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.65053</td></tr><tr><td>Train Loss</td><td>0.77415</td></tr><tr><td>Validation Accuracy</td><td>0.61617</td></tr><tr><td>Validation F1</td><td>0.60519</td></tr><tr><td>Validation Loss</td><td>0.8399</td></tr><tr><td>Validation Precision</td><td>0.62621</td></tr><tr><td>Validation Recall</td><td>0.61617</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/nmj4d2ci' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/nmj4d2ci</a><br> View project at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250811_164506-nmj4d2ci/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 17:47:36,750] Trial 1 finished with value: 0.6295664880662445 and parameters: {'learning_rate': 4.9786157368274445e-05, 'weight_decay': 2.230465513571302e-05, 'patience': 2, 'batch_size': 64, 'num_layers': 1}. Best is trial 0 with value: 0.6950803701899659.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_174739-dgn7nepx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dgn7nepx' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dgn7nepx' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dgn7nepx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▂▇▆▆▇█▇██</td></tr><tr><td>Validation F1</td><td>▁▂▇▆▆▇▇▇██</td></tr><tr><td>Validation Loss</td><td>█▆▁▁▁▂▃▆▄▇</td></tr><tr><td>Validation Precision</td><td>▁▂▆▆▆▇▇▇▇█</td></tr><tr><td>Validation Recall</td><td>▁▂▇▆▆▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.83334</td></tr><tr><td>Train Loss</td><td>0.39355</td></tr><tr><td>Validation Accuracy</td><td>0.67414</td></tr><tr><td>Validation F1</td><td>0.67313</td></tr><tr><td>Validation Loss</td><td>0.9757</td></tr><tr><td>Validation Precision</td><td>0.6813</td></tr><tr><td>Validation Recall</td><td>0.67414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dgn7nepx' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/dgn7nepx</a><br> View project at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250811_174739-dgn7nepx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 19:00:33,255] Trial 2 finished with value: 0.674135411592791 and parameters: {'learning_rate': 1.5072481269449607e-05, 'weight_decay': 7.010303283144758e-05, 'patience': 5, 'batch_size': 32, 'num_layers': 3}. Best is trial 0 with value: 0.6950803701899659.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_190035-hlv255sr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/hlv255sr' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/hlv255sr' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/hlv255sr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>Train Accuracy</td><td>▂▅▆▁▆█</td></tr><tr><td>Train Loss</td><td>█▄▃▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>██▂▁█▂</td></tr><tr><td>Validation F1</td><td>██▂▁█▂</td></tr><tr><td>Validation Loss</td><td>▇█▁▁▁▃</td></tr><tr><td>Validation Precision</td><td>██▂▁█▂</td></tr><tr><td>Validation Recall</td><td>██▂▁█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>Train Accuracy</td><td>0.21054</td></tr><tr><td>Train Loss</td><td>1.61375</td></tr><tr><td>Validation Accuracy</td><td>0.18607</td></tr><tr><td>Validation F1</td><td>0.05838</td></tr><tr><td>Validation Loss</td><td>1.62073</td></tr><tr><td>Validation Precision</td><td>0.03462</td></tr><tr><td>Validation Recall</td><td>0.18607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/hlv255sr' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/hlv255sr</a><br> View project at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250811_190035-hlv255sr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 19:44:24,560] Trial 3 finished with value: 0.27788602045786653 and parameters: {'learning_rate': 0.0005056541174507694, 'weight_decay': 5.831294042677587e-06, 'patience': 4, 'batch_size': 32, 'num_layers': 3}. Best is trial 0 with value: 0.6950803701899659.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250811_194433-xmg7b5t0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/xmg7b5t0' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/xmg7b5t0' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/xmg7b5t0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>Train Accuracy</td><td>▂▂▁▇▄▇▅█</td></tr><tr><td>Train Loss</td><td>█▅▃▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▆▁▂█▁▂▂█</td></tr><tr><td>Validation F1</td><td>▆▁▂█▁▂▂█</td></tr><tr><td>Validation Loss</td><td>▅█▄▂▂▁▁▂</td></tr><tr><td>Validation Precision</td><td>▆▁▂█▁▂▂█</td></tr><tr><td>Validation Recall</td><td>▆▁▂█▁▂▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>8</td></tr><tr><td>Train Accuracy</td><td>0.20984</td></tr><tr><td>Train Loss</td><td>1.61217</td></tr><tr><td>Validation Accuracy</td><td>0.27789</td></tr><tr><td>Validation F1</td><td>0.12086</td></tr><tr><td>Validation Loss</td><td>1.61374</td></tr><tr><td>Validation Precision</td><td>0.07722</td></tr><tr><td>Validation Recall</td><td>0.27789</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/xmg7b5t0' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment/runs/xmg7b5t0</a><br> View project at: <a href='https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment' target=\"_blank\">https://wandb.ai/diab55-tel-aviv-university/bertweet-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250811_194433-xmg7b5t0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 20:42:52,801] Trial 4 finished with value: 0.27788602045786653 and parameters: {'learning_rate': 0.00037662452485454344, 'weight_decay': 1.141876508702439e-06, 'patience': 3, 'batch_size': 32, 'num_layers': 3}. Best is trial 0 with value: 0.6950803701899659.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Validation Accuracy: 0.6951\n",
      "Best hyperparameters: {'learning_rate': 4.468780967980255e-05, 'weight_decay': 2.7116136370918185e-05, 'patience': 2, 'batch_size': 128, 'num_layers': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optuna_berttweet_accuracy_study.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# יצירת Study של Optuna - למקסם Validation Accuracy\n",
    "study = optuna.create_study(\n",
    "    study_name=\"BERT_Accuracy_Study\",\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "# הרצה של 5 ניסויים\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# הדפסת התוצאה הטובה ביותר\n",
    "print(f\"\\nBest Validation Accuracy: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# שמירת התוצאות (אופציונלי)\n",
    "joblib.dump(study, \"optuna_berttweet_accuracy_study.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8049830,
     "sourceId": 12735030,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
