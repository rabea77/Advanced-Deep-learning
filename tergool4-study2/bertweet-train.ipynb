{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-13T17:57:32.125304Z",
     "iopub.status.busy": "2025-08-13T17:57:32.125017Z",
     "iopub.status.idle": "2025-08-13T17:57:49.815964Z",
     "shell.execute_reply": "2025-08-13T17:57:49.815023Z",
     "shell.execute_reply.started": "2025-08-13T17:57:32.125275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrabeaotman\u001b[0m (\u001b[33mrabeaotman-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "wandb.login(key=\"017a8a1cf1968e847ba05f92a8935af78befe33f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:57:54.296704Z",
     "iopub.status.busy": "2025-08-13T17:57:54.296036Z",
     "iopub.status.idle": "2025-08-13T17:57:56.177783Z",
     "shell.execute_reply": "2025-08-13T17:57:56.176894Z",
     "shell.execute_reply.started": "2025-08-13T17:57:54.296677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → Extremely Negative\n",
      "1 → Extremely Positive\n",
      "2 → Negative\n",
      "3 → Neutral\n",
      "4 → Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib  # לשמירת ה-LabelEncoder (אופציונלי)\n",
    "\n",
    "# --- טעינת הדאטה ---\n",
    "df = pd.read_csv(\"/kaggle/input/bertweeteda2222/eda2.csv\")\n",
    "\n",
    "# שינוי שם עמודת הטקסט\n",
    "df = df.rename(columns={'clean_eda_2': 'Tweet'})\n",
    "\n",
    "# קידוד התוויות ממחרוזות למספרים\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Sentiment'])\n",
    "\n",
    "# שמירת המיפוי לשימוש עתידי (אופציונלי)\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# הצגת המיפוי\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} → {label}\")\n",
    "\n",
    "# --- חלוקה ל-Train / Eval /  ---\n",
    "train_df, eval_df = train_test_split(\n",
    "    df, test_size=0.3, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "\n",
    "# שמירה של רק העמודות הדרושות למודל\n",
    "train_df = train_df[['Tweet', 'label']]\n",
    "eval_df = eval_df[['Tweet', 'label']]\n",
    "\n",
    "\n",
    "# שמירת קבצים\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "eval_df.to_csv(\"eval_data.csv\", index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:57:59.425608Z",
     "iopub.status.busy": "2025-08-13T17:57:59.425285Z",
     "iopub.status.idle": "2025-08-13T17:58:33.853409Z",
     "shell.execute_reply": "2025-08-13T17:58:33.852300Z",
     "shell.execute_reply.started": "2025-08-13T17:57:59.425584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd95684875fb4f489337a3940a9630ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68a517dd2546b8a89014701afcf89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addb3d085ca44c459bc0165a9c634437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ec2fd1022e4a2baf60ee2982c1f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:58:11.749525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755107891.941645      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755107891.997412      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e604ba137649cc83837fe97c46e548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db125fd0a54e43b12af18e42d724a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#לשקול להחזיר את אמוג'ים, hashtags, mentions############\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# נשתמש במודל BERTweet-Large של VinAI\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "\n",
    "# חשוב! BERTweet לא תומך בטוקניזר המהיר (Fast tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# טען את המודל למטרת Classification עם 5 תוויות\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
    "\n",
    "model  # הצגת מבנה המודל\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:23.218682Z",
     "iopub.status.busy": "2025-08-13T17:59:23.217847Z",
     "iopub.status.idle": "2025-08-13T17:59:23.224405Z",
     "shell.execute_reply": "2025-08-13T17:59:23.223523Z",
     "shell.execute_reply.started": "2025-08-13T17:59:23.218653Z"
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.texts = dataframe['Tweet'].tolist()\n",
    "        self.labels = dataframe['label'].tolist() \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=80,  \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:28.204866Z",
     "iopub.status.busy": "2025-08-13T17:59:28.204554Z",
     "iopub.status.idle": "2025-08-13T17:59:28.210006Z",
     "shell.execute_reply": "2025-08-13T17:59:28.209174Z",
     "shell.execute_reply.started": "2025-08-13T17:59:28.204844Z"
    }
   },
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_acc, best_acc_epoch, current_acc, current_epoch):\n",
    "    \"\"\"\n",
    "    עצירה מוקדמת לפי Val Accuracy בלבד.\n",
    "    מחזיר: best_acc, best_acc_epoch, early_stop_flag\n",
    "    \"\"\"\n",
    "    early_stop_flag = False\n",
    "    if current_acc > best_acc:\n",
    "        best_acc = current_acc\n",
    "        best_acc_epoch = current_epoch\n",
    "    elif current_epoch - best_acc_epoch > patience:\n",
    "        early_stop_flag = True\n",
    "    return best_acc, best_acc_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:30.529620Z",
     "iopub.status.busy": "2025-08-13T17:59:30.528807Z",
     "iopub.status.idle": "2025-08-13T17:59:30.543462Z",
     "shell.execute_reply": "2025-08-13T17:59:30.542478Z",
     "shell.execute_reply.started": "2025-08-13T17:59:30.529590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2 3 4]\n",
      "Class weights: [1.49386583 1.23687054 0.82821997 1.08390152 0.71833814]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# חישוב משקלי תוויות לפי הופעה בפועל — על ה-TRAIN בלבד\n",
    "train_labels = train_df['label'].values\n",
    "classes = np.unique(train_labels)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# הפיכה לטנסור לשימוש בתוך CrossEntropyLoss\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "# פונקציית הפסד עם משקלים\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# אופציונלי: להדפיס כדי לדעת מה קיבלת\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:33.609709Z",
     "iopub.status.busy": "2025-08-13T17:59:33.609381Z",
     "iopub.status.idle": "2025-08-13T17:59:33.621939Z",
     "shell.execute_reply": "2025-08-13T17:59:33.621020Z",
     "shell.execute_reply.started": "2025-08-13T17:59:33.609686Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_accuracy_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            total_train_samples += input_ids.size(0)\n",
    "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * input_ids.size(0)\n",
    "                total_val_samples += input_ids.size(0)\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "            patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch)\n",
    "\n",
    "        if val_accuracy == best_val_accuracy:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Validation F1\": val_f1\n",
    "        })\n",
    "\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
    "\n",
    "    return best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:39.710894Z",
     "iopub.status.busy": "2025-08-13T17:59:39.710556Z",
     "iopub.status.idle": "2025-08-13T17:59:39.719896Z",
     "shell.execute_reply": "2025-08-13T17:59:39.718907Z",
     "shell.execute_reply.started": "2025-08-13T17:59:39.710870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    # === Hyperparameter suggestions ===\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 2, 5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "\n",
    "    # === Tokenizer and Dataset ===\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", use_fast=False)\n",
    "\n",
    "    train_dataset = TweetDataset(train_df, tokenizer)\n",
    "    val_dataset = TweetDataset(eval_df, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # === Load BERTweet Model ===\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"vinai/bertweet-large\", num_labels=5\n",
    "    ).to(device)\n",
    "\n",
    "    # === Freeze all layers first ===\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # === Unfreeze the last `num_layers` of encoder ===\n",
    "    for param in model.roberta.encoder.layer[-num_layers:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # === Unfreeze the classification head ===\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # === Define loss with class weights ===\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # === Optimizer ===\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # === Initialize W&B for tracking ===\n",
    "    wandb.init(\n",
    "        project=\"bertweet-sentiment222222\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"patience\": patience,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"architecture\": \"BERTweet-Large\",\n",
    "            \"dataset\": \"covid-tweets\"\n",
    "        },\n",
    "        name=f\"trial_{trial.number}\"\n",
    "    )\n",
    "\n",
    "    # === Train and evaluate ===\n",
    "    best_val_accuracy = train_model_with_hyperparams(\n",
    "        model, train_loader, val_loader, optimizer, criterion, epochs=10,\n",
    "        patience=patience, trial=trial\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T17:59:47.535713Z",
     "iopub.status.busy": "2025-08-13T17:59:47.535405Z",
     "iopub.status.idle": "2025-08-13T22:22:42.805371Z",
     "shell.execute_reply": "2025-08-13T22:22:42.804557Z",
     "shell.execute_reply.started": "2025-08-13T17:59:47.535691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 17:59:47,537] A new study created in memory with name: BERT_Accuracy_Study\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_175950-566bukqw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/566bukqw' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/566bukqw' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/566bukqw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▆▆▇▇▇▇█▇</td></tr><tr><td>Validation F1</td><td>▁▄▆▆▇▇▇██▇</td></tr><tr><td>Validation Loss</td><td>█▅▃▁▂▃▂▃▃▅</td></tr><tr><td>Validation Precision</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>Validation Recall</td><td>▁▄▆▆▇▇▇▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.85221</td></tr><tr><td>Train Loss</td><td>0.35532</td></tr><tr><td>Validation Accuracy</td><td>0.70083</td></tr><tr><td>Validation F1</td><td>0.70017</td></tr><tr><td>Validation Loss</td><td>0.84682</td></tr><tr><td>Validation Precision</td><td>0.70782</td></tr><tr><td>Validation Recall</td><td>0.70083</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/566bukqw' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/566bukqw</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250813_175950-566bukqw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 18:50:33,357] Trial 0 finished with value: 0.7159980430528375 and parameters: {'learning_rate': 0.00020529433694267552, 'weight_decay': 1.1640070798492136e-06, 'patience': 4, 'batch_size': 64, 'num_layers': 2}. Best is trial 0 with value: 0.7159980430528375.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_185036-jvus1j3g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/jvus1j3g' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/jvus1j3g' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/jvus1j3g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▅▅▄▃▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▂▅▅▆▆▇▆▇█</td></tr><tr><td>Validation F1</td><td>▁▂▅▅▅▇▇▆▇█</td></tr><tr><td>Validation Loss</td><td>█▅▃▃▂▂▁▂▂▃</td></tr><tr><td>Validation Precision</td><td>▁▃▅▅▆▇▇▇▇█</td></tr><tr><td>Validation Recall</td><td>▁▂▅▅▆▆▇▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.77714</td></tr><tr><td>Train Loss</td><td>0.5164</td></tr><tr><td>Validation Accuracy</td><td>0.67221</td></tr><tr><td>Validation F1</td><td>0.66951</td></tr><tr><td>Validation Loss</td><td>0.86462</td></tr><tr><td>Validation Precision</td><td>0.67917</td></tr><tr><td>Validation Recall</td><td>0.67221</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/jvus1j3g' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/jvus1j3g</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250813_185036-jvus1j3g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 19:42:32,062] Trial 1 finished with value: 0.6722113502935421 and parameters: {'learning_rate': 1.6583729518845456e-05, 'weight_decay': 1.9331962002312085e-05, 'patience': 5, 'batch_size': 32, 'num_layers': 2}. Best is trial 0 with value: 0.7159980430528375.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_194236-t5kjq7y0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/t5kjq7y0' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/t5kjq7y0' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/t5kjq7y0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▇█▇█▇██</td></tr><tr><td>Validation F1</td><td>▁▅▅▇█▇█▇██</td></tr><tr><td>Validation Loss</td><td>█▄▃▁▁▃▃▅▇▇</td></tr><tr><td>Validation Precision</td><td>▁▄▅▆█▇█▇█▇</td></tr><tr><td>Validation Recall</td><td>▁▄▅▇█▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.90501</td></tr><tr><td>Train Loss</td><td>0.23359</td></tr><tr><td>Validation Accuracy</td><td>0.72505</td></tr><tr><td>Validation F1</td><td>0.72163</td></tr><tr><td>Validation Loss</td><td>0.91168</td></tr><tr><td>Validation Precision</td><td>0.72549</td></tr><tr><td>Validation Recall</td><td>0.72505</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/t5kjq7y0' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/t5kjq7y0</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250813_194236-t5kjq7y0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 20:36:28,266] Trial 2 finished with value: 0.7348336594911937 and parameters: {'learning_rate': 0.00013216020928849163, 'weight_decay': 8.692956783751181e-06, 'patience': 2, 'batch_size': 64, 'num_layers': 3}. Best is trial 2 with value: 0.7348336594911937.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_203630-qn6c6ll9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/qn6c6ll9' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/qn6c6ll9' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/qn6c6ll9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▆▄▆▆▇▇▆██</td></tr><tr><td>Validation F1</td><td>▁▆▅▆▆█▇▆█▇</td></tr><tr><td>Validation Loss</td><td>█▂▃▁▂▂▂▂▁▂</td></tr><tr><td>Validation Precision</td><td>▁▅▄▆▆█▇▆██</td></tr><tr><td>Validation Recall</td><td>▁▆▄▆▆▇▇▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.72518</td></tr><tr><td>Train Loss</td><td>0.62286</td></tr><tr><td>Validation Accuracy</td><td>0.66585</td></tr><tr><td>Validation F1</td><td>0.66147</td></tr><tr><td>Validation Loss</td><td>0.83086</td></tr><tr><td>Validation Precision</td><td>0.67126</td></tr><tr><td>Validation Recall</td><td>0.66585</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/qn6c6ll9' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/qn6c6ll9</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250813_203630-qn6c6ll9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 21:27:18,392] Trial 3 finished with value: 0.675880626223092 and parameters: {'learning_rate': 0.00026428127993442503, 'weight_decay': 1.8234780031599637e-05, 'patience': 2, 'batch_size': 64, 'num_layers': 2}. Best is trial 2 with value: 0.7348336594911937.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250813_212720-a42bqw5g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/a42bqw5g' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/a42bqw5g' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/a42bqw5g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▃▃▁▄▁▄▅█▇▄</td></tr><tr><td>Train Loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▃▆▂▃█▃▆█</td></tr><tr><td>Validation F1</td><td>▁▁▃▆▂▃█▃▆█</td></tr><tr><td>Validation Loss</td><td>█▂▂▁▂▁▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▁▃▆▂▃█▃▆█</td></tr><tr><td>Validation Recall</td><td>▁▁▃▆▂▃█▃▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.20626</td></tr><tr><td>Train Loss</td><td>1.61175</td></tr><tr><td>Validation Accuracy</td><td>0.27862</td></tr><tr><td>Validation F1</td><td>0.12143</td></tr><tr><td>Validation Loss</td><td>1.61036</td></tr><tr><td>Validation Precision</td><td>0.07763</td></tr><tr><td>Validation Recall</td><td>0.27862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/a42bqw5g' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222/runs/a42bqw5g</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/bertweet-sentiment222222</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250813_212720-a42bqw5g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 22:22:42,796] Trial 4 finished with value: 0.27862035225048926 and parameters: {'learning_rate': 0.0007727621497451943, 'weight_decay': 2.7017647851856643e-06, 'patience': 3, 'batch_size': 32, 'num_layers': 3}. Best is trial 2 with value: 0.7348336594911937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Validation Accuracy: 0.7348\n",
      "Best hyperparameters: {'learning_rate': 0.00013216020928849163, 'weight_decay': 8.692956783751181e-06, 'patience': 2, 'batch_size': 64, 'num_layers': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optuna_berttweet_accuracy_study.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# יצירת Study של Optuna - למקסם Validation Accuracy\n",
    "study = optuna.create_study(\n",
    "    study_name=\"BERT_Accuracy_Study\",\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "# הרצה של 5 ניסויים\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# הדפסת התוצאה הטובה ביותר\n",
    "print(f\"\\nBest Validation Accuracy: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# שמירת התוצאות (אופציונלי)\n",
    "joblib.dump(study, \"optuna_berttweet_accuracy_study.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8064433,
     "sourceId": 12756739,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
