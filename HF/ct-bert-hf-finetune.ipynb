{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12788188,"sourceType":"datasetVersion","datasetId":8085117},{"sourceId":529722,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":414380,"modelId":432130}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-21T09:14:17.456406Z","iopub.execute_input":"2025-08-21T09:14:17.456675Z","iopub.status.idle":"2025-08-21T09:14:22.709149Z","shell.execute_reply.started":"2025-08-21T09:14:17.456654Z","shell.execute_reply":"2025-08-21T09:14:22.708146Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# 0) Imports & basic setup\n# =========================\nimport os, math\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\n\nimport evaluate\nfrom tqdm.auto import tqdm\nimport wandb\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# Reproducibility (optional)\nSEED = 42\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n\nwandb.login(key=\"017a8a1cf1968e847ba05f92a8935af78befe33f\")\n\nBASE_MODEL_OR_DIR = \"digitalepidemiologylab/covid-twitter-bert\"","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:14:27.978577Z","iopub.execute_input":"2025-08-21T09:14:27.978860Z","iopub.status.idle":"2025-08-21T09:15:06.667726Z","shell.execute_reply.started":"2025-08-21T09:14:27.978834Z","shell.execute_reply":"2025-08-21T09:15:06.666914Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-08-21 09:14:43.357304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755767683.591491      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755767683.654665      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrabeaotman\u001b[0m (\u001b[33mrabeaotman-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"OLD_CKPT_DIR = \"/kaggle/input/covidbertttttttttt/pytorch/default/1\"   \nNUM_LABELS   = 5  \nIGNORE_HEAD_MISMATCH = False\n\ntokenizer = AutoTokenizer.from_pretrained(OLD_CKPT_DIR, use_fast=True)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    OLD_CKPT_DIR,\n    num_labels=NUM_LABELS,\n    ignore_mismatched_sizes=IGNORE_HEAD_MISMATCH\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:15:59.263050Z","iopub.execute_input":"2025-08-21T09:15:59.263712Z","iopub.status.idle":"2025-08-21T09:16:11.782641Z","shell.execute_reply.started":"2025-08-21T09:15:59.263688Z","shell.execute_reply":"2025-08-21T09:16:11.781725Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport random, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder   # <-- this fixes your error\nfrom transformers import set_seed\nDATA_PATH = \"/kaggle/input/edaaaaa-tergol5/my_eda.csv\"\nTEXT_COL   = \"Tweet\"        # Text column after your EDA\nTARGET_COL = \"Sentiment\"    # String labels\nSEED = 42\nset_seed(SEED)\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\n\n# --------- Load & encode labels ---------\ndf = pd.read_csv(DATA_PATH, encoding=\"latin1\")\ndf = df.rename(columns={'Tweet': 'Original'})\ndf = df.rename(columns={'normalized_tweet': 'Tweet'})  # this becomes the text we train on\nassert TEXT_COL in df.columns and TARGET_COL in df.columns, f\"Missing {TEXT_COL}/{TARGET_COL} in CSV.\"\n\n\nle = LabelEncoder()\ndf[\"label\"] = le.fit_transform(df[TARGET_COL])\nnum_labels = df[\"label\"].nunique()\nprint(\"Labels:\", dict(enumerate(le.classes_)))\nprint(\"num_labels =\", num_labels)\n\n# Stratified split\ntrain_df, eval_df = train_test_split(\n    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n)\nlen(train_df), len(eval_df)","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:16:15.682970Z","iopub.execute_input":"2025-08-21T09:16:15.683276Z","iopub.status.idle":"2025-08-21T09:16:16.150000Z","shell.execute_reply.started":"2025-08-21T09:16:15.683252Z","shell.execute_reply":"2025-08-21T09:16:16.149286Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Labels: {0: 'Extremely Negative', 1: 'Extremely Positive', 2: 'Negative', 3: 'Neutral', 4: 'Positive'}\nnum_labels = 5\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(32873, 8219)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"######bessssssttttttt paraaaaaaaaaams\n\nbest_params = {'learning_rate': 1.5252567328044727e-05,\n'weight_decay': 5.256377166554879e-06,\n'warmup_ratio': 0.13130278878798132,\n'max_length': 96,\n'train_bs': 16,\n'grad_acc': 1,\n'label_smoothing': 0.02939616441539268,\n'gradient_checkpointing': True}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T09:16:19.357928Z","iopub.execute_input":"2025-08-21T09:16:19.358413Z","iopub.status.idle":"2025-08-21T09:16:19.362123Z","shell.execute_reply.started":"2025-08-21T09:16:19.358388Z","shell.execute_reply":"2025-08-21T09:16:19.361607Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# =========================\n# 2) Build HF DatasetDict\n# =========================\n# Keep only the columns we need for training: text + numeric label\ncols = [TEXT_COL, \"label\"]\ntrain_ds = Dataset.from_pandas(train_df[cols].rename(columns={\"label\": \"labels\"}), preserve_index=False)\neval_ds  = Dataset.from_pandas(eval_df[cols].rename(columns={\"label\": \"labels\"}),  preserve_index=False)\n\n# Use keys that match your earlier code (train/test)\nraw_datasets = DatasetDict({\"train\": train_ds, \"test\": eval_ds})\nraw_datasets\n\n# =========================\n# 3) Tokenizer & tokenization\n# =========================\ntokenizer = AutoTokenizer.from_pretrained(OLD_CKPT_DIR, use_fast=True)\n\ndef tokenize_fn(batch):\n    return tokenizer(\n        batch[TEXT_COL],\n        truncation=True,\n        max_length=best_params['max_length'],   # or remove to use dynamic truncation\n    )\n\ntokenized_datasets = raw_datasets.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[TEXT_COL],   # drop raw text; keep only tokenized fields + \"labels\"\n)\n\n# Make sure the column is named exactly \"labels\" (HF models expect this)\nassert \"labels\" in tokenized_datasets[\"train\"].column_names\n\n# Set torch format for PyTorch DataLoader downstream\ntokenized_datasets.set_format(\n    type=\"torch\",\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n)\ntokenized_datasets\n","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:16:21.079789Z","iopub.execute_input":"2025-08-21T09:16:21.080515Z","iopub.status.idle":"2025-08-21T09:16:26.201361Z","shell.execute_reply.started":"2025-08-21T09:16:21.080480Z","shell.execute_reply":"2025-08-21T09:16:26.200638Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0cbadc4a794a4cac382e23b6448c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8219 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1630292fdd04368854fca7572fcbb43"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 32873\n    })\n    test: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 8219\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import set_seed, AutoTokenizer, DataCollatorWithPadding\n# =========================\n# 4) Dataloaders (same pattern you use)\n# =========================\nfrom torch.utils.data import DataLoader\n\n# Optional: pad to multiples of 8 for better Tensor Core utilization on GPUs\ncollator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    pad_to_multiple_of=8 if torch.cuda.is_available() else None\n)\n\n# Small subsets exactly like your snippet\ntrain_dataset = tokenized_datasets[\"train\"].shuffle(seed=SEED)\n\n\neval_dataset  = tokenized_datasets[\"test\"].shuffle(seed=SEED)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=best_params['train_bs'], collate_fn=collator)\neval_dataloader  = DataLoader(eval_dataset, batch_size=best_params['train_bs'], collate_fn=collator)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:16:28.641167Z","iopub.execute_input":"2025-08-21T09:16:28.641463Z","iopub.status.idle":"2025-08-21T09:16:28.662749Z","shell.execute_reply.started":"2025-08-21T09:16:28.641442Z","shell.execute_reply":"2025-08-21T09:16:28.662030Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# 4) Optimizer & Scheduler (same pattern)\n# ============================================\nlearning_rate = best_params['learning_rate']\nnum_epochs = 3\n\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay = best_params[\"weight_decay\"])\n\nnum_training_steps = num_epochs * len(train_dataloader)\n\n# You can add warmup steps if you like (e.g., 10% of total steps)\nwarmup_steps = int(best_params[\"warmup_ratio\"] * num_training_steps)\n\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=num_training_steps\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:16:31.399076Z","iopub.execute_input":"2025-08-21T09:16:31.399367Z","iopub.status.idle":"2025-08-21T09:16:31.406189Z","shell.execute_reply.started":"2025-08-21T09:16:31.399344Z","shell.execute_reply":"2025-08-21T09:16:31.405493Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# =========================\n# Train → Eval → Save + W&B\n# =========================\nimport os, time\nfrom tqdm.auto import tqdm\nimport evaluate\nimport torch\n\n\n# ----- W&B setup -----\nWANDB_PROJECT = \"finetune-hf-ct-bert\"   # <-- change if you want\nRUN_NAME      = \"last take\"\n\n# If your API key is in the env (recommended on Kaggle), this will pick it up automatically.\nwandb.login()\nwandb.init(project=WANDB_PROJECT, name=RUN_NAME, config={\n    \"num_epochs\": num_epochs,\n    \"train_batch_size\": train_dataloader.batch_size,\n    \"eval_batch_size\": eval_dataloader.batch_size,\n    \"max_steps\": num_training_steps,\n    \"model_name_or_path\": (MODEL_DIR if 'MODEL_DIR' in globals() else \"unknown\"),\n})\n\n# Optional: track gradients/weights; set log_freq to every N steps\nwandb.watch(model, log=\"gradients\", log_freq=50)\n\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\nprogress_bar = tqdm(range(num_training_steps))\n\nglobal_step = 0\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        lr_scheduler.step()\n\n        # --- Logging per step ---\n        lr_now = lr_scheduler.get_last_lr()[0] if hasattr(lr_scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"]\n        wandb.log({\n            \"train/step_loss\": float(loss.item()),\n            \"train/lr\": float(lr_now),\n            \"train/epoch\": epoch,\n            \"train/global_step\": global_step,\n        }, step=global_step)\n\n        epoch_loss += loss.item()\n        progress_bar.update(1)\n        global_step += 1\n\n    avg = epoch_loss / max(1, len(train_dataloader))\n    print(f\"Epoch {epoch+1}/{num_epochs} - train_loss: {avg:.4f}\")\n    # --- Logging per epoch ---\n    wandb.log({\"train/epoch_loss\": float(avg), \"train/epoch_end\": epoch}, step=global_step)\n\n# =========================\n# Evaluation (accuracy)\n# =========================\nmetric = evaluate.load(\"accuracy\")\nmodel.eval()\neval_loss = 0.0\n\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n\n        # get predictions\n        preds = torch.argmax(outputs.logits, dim=-1)\n        metric.add_batch(predictions=preds, references=batch[\"labels\"])\n\n        # accumulate loss\n        eval_loss += outputs.loss.item()\n\n# average loss\navg_eval_loss = eval_loss / max(1, len(eval_dataloader))\n\n# accuracy\nacc = metric.compute()  # {'accuracy': ...}\n\nprint(f\"Final Eval Accuracy: {acc['accuracy']:.4f}\")\nprint(f\"Final Eval Loss: {avg_eval_loss:.4f}\")\n\nwandb.log({\n    \"eval/accuracy\": float(acc[\"accuracy\"]),\n    \"eval/loss\": float(avg_eval_loss)\n}, step=global_step)\n\n\n# =========================\n# Save checkpoint (local)\n# =========================\nSAVE_DIR = \"/kaggle/working/finetuned_from_old\"\nos.makedirs(SAVE_DIR, exist_ok=True)\nmodel.save_pretrained(SAVE_DIR, safe_serialization=True)\ntokenizer.save_pretrained(SAVE_DIR)\nprint(\"Model & tokenizer saved to:\", SAVE_DIR)\n\n# =========================\n# Log model as a W&B artifact\n# =========================\nartifact = wandb.Artifact(\n    name=\"finetuned-from-old\",\n    type=\"model\",\n    metadata={\"accuracy\": float(acc[\"accuracy\"]), \"epochs\": num_epochs}\n)\nartifact.add_dir(SAVE_DIR)\nwandb.log_artifact(artifact)\n\nwandb.finish()\n","metadata":{"execution":{"iopub.status.busy":"2025-08-21T09:16:39.073672Z","iopub.execute_input":"2025-08-21T09:16:39.073943Z","iopub.status.idle":"2025-08-21T10:05:52.340188Z","shell.execute_reply.started":"2025-08-21T09:16:39.073924Z","shell.execute_reply":"2025-08-21T10:05:52.339680Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250821_091639-k5qy2ihz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert/runs/k5qy2ihz' target=\"_blank\">last take</a></strong> to <a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert/runs/k5qy2ihz' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert/runs/k5qy2ihz</a>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2473381169.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88017be5be554ccd922364f473d8d40f"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2473381169.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - train_loss: 0.2405\nEpoch 2/3 - train_loss: 0.1651\nEpoch 3/3 - train_loss: 0.0766\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f8b3fec6e3a4266b11b1a40f15fcac0"}},"metadata":{}},{"name":"stdout","text":"Final Eval Accuracy: 0.8884\nFinal Eval Loss: 0.4125\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working/finetuned_from_old)... ","output_type":"stream"},{"name":"stdout","text":"Model & tokenizer saved to: /kaggle/working/finetuned_from_old\n","output_type":"stream"},{"name":"stderr","text":"Done. 4.6s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅████████████████</td></tr><tr><td>train/epoch_end</td><td>▁▅█</td></tr><tr><td>train/epoch_loss</td><td>█▅▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/lr</td><td>▂▂▄▄▅██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▁▁▁▁</td></tr><tr><td>train/step_loss</td><td>▄▅▂▂▂▂▂▆▂▆█▅▃▇▃▃▁▂▃▂▂▅▁▂▃▅▁▂▂▁▁▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88843</td></tr><tr><td>eval/loss</td><td>0.41252</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/epoch_end</td><td>2</td></tr><tr><td>train/epoch_loss</td><td>0.0766</td></tr><tr><td>train/global_step</td><td>6164</td></tr><tr><td>train/lr</td><td>0</td></tr><tr><td>train/step_loss</td><td>0.00228</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">last take</strong> at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert/runs/k5qy2ihz' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert/runs/k5qy2ihz</a><br> View project at: <a href='https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert' target=\"_blank\">https://wandb.ai/rabeaotman-tel-aviv-university/finetune-hf-ct-bert</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250821_091639-k5qy2ihz/logs</code>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Go to the parent directory\n!cd /kaggle/working && zip -r finetuned_from_old.zip finetuned_from_old\n\n# This creates /kaggle/working/finetuned_from_old.zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T10:06:00.469781Z","iopub.execute_input":"2025-08-21T10:06:00.470386Z","iopub.status.idle":"2025-08-21T10:07:09.279787Z","shell.execute_reply.started":"2025-08-21T10:06:00.470363Z","shell.execute_reply":"2025-08-21T10:07:09.278977Z"}},"outputs":[{"name":"stdout","text":"  adding: finetuned_from_old/ (stored 0%)\n  adding: finetuned_from_old/config.json (deflated 54%)\n  adding: finetuned_from_old/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n  adding: finetuned_from_old/vocab.txt (deflated 53%)\n  adding: finetuned_from_old/special_tokens_map.json (deflated 80%)\n  adding: finetuned_from_old/tokenizer_config.json (deflated 74%)\n  adding: finetuned_from_old/tokenizer.json (deflated 71%)\n","output_type":"stream"}],"execution_count":10}]}